#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    PAYROLL REGULATORY AUDIT AGENT v7.0                       ‚ïë
‚ïë                          "The Ultimate Scanner"                               ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  Features:                                                                    ‚ïë
‚ïë  ‚Ä¢ Async parallel processing (10x faster)                                    ‚ïë
‚ïë  ‚Ä¢ Intelligent document detection with ML-like scoring                       ‚ïë
‚ïë  ‚Ä¢ Smart rate limiting per domain                                            ‚ïë
‚ïë  ‚Ä¢ PDF text extraction + OCR fallback indicator                              ‚ïë
‚ïë  ‚Ä¢ Gemini batch analysis with retry logic                                    ‚ïë
‚ïë  ‚Ä¢ Rich Telegram reports with categorization                                 ‚ïë
‚ïë  ‚Ä¢ SQLite with full audit trail                                              ‚ïë
‚ïë  ‚Ä¢ Comprehensive government repository coverage                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import asyncio
import aiohttp
import sqlite3
import os
import re
import io
import json
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, parse_qs
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Tuple, Set
from enum import Enum
from collections import defaultdict
import time

# Sync imports for specific operations
import requests
from bs4 import BeautifulSoup

# Optional PDF support
try:
    from pypdf import PdfReader
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    print("‚ö†Ô∏è  pypdf not installed. Run: pip install pypdf")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Config:
    """Central configuration"""
    # API Keys (from environment)
    GEMINI_API_KEY = os.environ.get("GEMINI_KEY", "")
    TELEGRAM_TOKEN = os.environ.get("AUDIT_BOT_TOKEN", "")
    TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID", "")

    # Database
    DB_PATH = "payroll_audit_v7.db"

    # Scanning
    MAX_CONCURRENT_REQUESTS = 10
    REQUEST_TIMEOUT = 30
    RATE_LIMIT_PER_DOMAIN = 2.0  # seconds between requests to same domain
    MAX_DOCUMENTS_PER_REPO = 50
    LOOKBACK_DAYS = 180  # 6 months

    # Content
    MAX_PDF_PAGES = 3
    MAX_PDF_SIZE_MB = 15
    MAX_CONTENT_LENGTH = 5000

    # Gemini
    GEMINI_MODEL = "gemini-1.5-flash"
    GEMINI_BATCH_SIZE = 5
    GEMINI_RETRY_ATTEMPTS = 3
    GEMINI_RETRY_DELAY = 2

    @classmethod
    def validate(cls):
        missing = []
        if not cls.GEMINI_API_KEY:
            missing.append("GEMINI_KEY")
        if not cls.TELEGRAM_TOKEN:
            missing.append("AUDIT_BOT_TOKEN")
        if not cls.TELEGRAM_CHAT_ID:
            missing.append("TELEGRAM_CHAT_ID")

        if missing:
            print(f"‚ùå Missing environment variables: {', '.join(missing)}")
            return False
        return True

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATA MODELS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DocumentCategory(Enum):
    TAX = "tax"
    LABOR = "labor"
    PENSION = "pension"
    SOCIAL_SECURITY = "social_security"
    COMPLIANCE = "compliance"
    OTHER = "other"

@dataclass
class Repository:
    """Government repository configuration"""
    url: str
    country: str
    agency: str
    doc_type: str = "general"  # circular, notification, press_release, etc.
    selectors: Dict = field(default_factory=dict)  # CSS selectors for content

@dataclass
class Document:
    """Extracted document"""
    url: str
    title: str
    country: str
    agency: str
    date_found: Optional[str] = None
    date_published: Optional[str] = None
    doc_id: Optional[str] = None
    content_snippet: Optional[str] = None
    is_pdf: bool = False
    relevance_score: float = 0.0
    category: DocumentCategory = DocumentCategory.OTHER
    ai_summary: Optional[str] = None
    is_relevant: bool = False

    def __hash__(self):
        return hash(self.url)

    def __eq__(self, other):
        return self.url == other.url

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PATTERN MATCHING (Pre-compiled for performance)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Patterns:
    """Pre-compiled regex patterns"""

    # Date patterns
    DATES = [
        re.compile(r'(\d{1,2})[./-](\d{1,2})[./-](20\d{2})'),
        re.compile(r'(20\d{2})[./-](\d{1,2})[./-](\d{1,2})'),
        re.compile(r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(20\d{2})', re.I),
        re.compile(r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December),?\s+(20\d{2})', re.I),
        re.compile(r'dated?\s*:?\s*(\d{1,2}[./-]\d{1,2}[./-]20\d{2})', re.I),
    ]

    # Document ID patterns (circular numbers, notification numbers, etc.)
    DOC_IDS = [
        re.compile(r'(?:circular|notification|order|memo|advisory|resolution)\s*(?:no\.?|number|#)\s*([\w./-]+)', re.I),
        re.compile(r'(?:RMC|RMO|RR|DA|DO|LA)\s*(?:No\.?)?\s*([\d-]+)', re.I),  # Philippines
        re.compile(r'F\.?\s*No\.?\s*([\d/.-]+)', re.I),  # India
        re.compile(r'(?:No\.?|Number)\s*([\d]+[/-][\d]+(?:[/-][\d]+)?)', re.I),
        re.compile(r'(?:S\.?O\.?|G\.?S\.?R\.?)\s*(\d+)', re.I),  # India gazette
        re.compile(r'\b(\d{1,4}[/-]20\d{2})\b'),  # Generic: 123/2024
        re.compile(r'\b([A-Z]{2,5}[-/]?\d{2,5}[-/]?20\d{2})\b'),  # CODE-123-2024
    ]

    # Year pattern
    YEAR = re.compile(r'\b(202[3-6])\b')

    # File extensions
    FILE_EXT = re.compile(r'\.(pdf|doc|docx|xls|xlsx|rtf)(?:\?|$)', re.I)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BLOCKLIST & ALLOWLIST
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Filters:
    """Document filtering logic"""

    # Navigation/UI elements to BLOCK
    NAVIGATION_BLOCKLIST = frozenset([
        # Generic navigation
        'home', 'about', 'about us', 'contact', 'contact us', 'search', 'login',
        'logout', 'register', 'sign in', 'sign up', 'sign out', 'privacy',
        'privacy policy', 'terms', 'terms of service', 'terms and conditions',
        'sitemap', 'site map', 'cookie', 'cookies', 'disclaimer', 'legal',
        'help', 'faq', 'faqs', 'support', 'feedback',

        # UI elements
        'skip to content', 'skip to main content', 'skip navigation', 'main content',
        'screen reader', 'accessible mode', 'accessibility', 'text size', 'font size',
        'high contrast', 'print', 'share', 'email', 'tweet', 'facebook', 'twitter',
        'linkedin', 'instagram', 'youtube', 'social media', 'follow us', 'subscribe',
        'newsletter', 'logo', 'banner', 'header', 'footer', 'menu', 'navigation',
        'breadcrumb', 'back to top', 'scroll to top', 'read more', 'learn more',
        'click here', 'view all', 'see all', 'show more', 'load more', 'next',
        'previous', 'first', 'last', 'page', 'pagination',

        # Language selectors
        'english', 'hindi', '‡§π‡§ø‡§Ç‡§¶‡•Ä', 'arabic', 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'fran√ßais', 'espa√±ol',
        'select language', 'change language', 'translate',

        # Generic sections
        'services', 'all services', 'our services', 'products', 'solutions',
        'resources', 'downloads', 'documents', 'publications', 'media', 'news',
        'events', 'calendar', 'gallery', 'photos', 'videos', 'blog', 'articles',
        'press room', 'media center', 'newsroom', 'careers', 'jobs', 'vacancies',
        'recruitment', 'opportunities', 'work with us', 'tenders', 'bids',
        'procurement', 'auction', 'rfp', 'rfq', 'eoi',

        # Organization structure
        'who we are', 'leadership', 'management', 'board', 'directors', 'team',
        'staff', 'employees', 'departments', 'divisions', 'branches', 'offices',
        'locations', 'locate', 'find us', 'visit us', 'address', 'map', 'directions',
        'mission', 'vision', 'values', 'goals', 'objectives', 'history', 'milestones',

        # User types/portals
        'for employers', 'for employees', 'for individuals', 'for businesses',
        'for companies', 'for citizens', 'for taxpayers', 'for members',
        'employer services', 'employee services', 'citizen services',

        # Specific junk patterns
        'help desk', 'toll free', 'tollfree', 'call center', 'customer care',
        'who\'s who', 'organogram', 'organization chart', 'photo albums',
        'awareness workshops', 'service centres', 'taxpayer service',
        'training institutes', 'capacity building',

        # Portal navigation (not documents)
        'about pin', 'pin registration', 'pin dormancy', 'pin cancellation',
        'how to register', 'how to file', 'how to pay', 'how to print',
        'requirements for', 'procedures for', 'guidelines for',
        'tax registration', 'tax obligations', 'tax types', 'tax rates',
        'taxpayer segments', 'e-commerce', 'e-filing', 'e-payment', 'online services',

        # Category pages (not actual documents)
        'circulars', 'notifications', 'orders', 'resolutions', 'regulations',
        'acts', 'laws', 'rules', 'guidelines', 'manuals', 'handbooks',
        'forms', 'templates', 'formats', 'public notices', 'press releases',
    ])

    # Document type keywords that indicate relevance
    DOCUMENT_KEYWORDS = frozenset([
        'circular', 'notification', 'order', 'resolution', 'memo', 'memorandum',
        'advisory', 'guideline', 'directive', 'gazette', 'notice', 'announcement',
        'amendment', 'act', 'bill', 'rule', 'regulation', 'ordinance', 'decree',
        'press release', 'public notice', 'practice note', 'interpretation note',
        'revenue memorandum', 'labor advisory', 'tax advisory',
    ])

    # Regulatory topic keywords
    REGULATORY_KEYWORDS = frozenset([
        # Tax
        'income tax', 'corporate tax', 'vat', 'gst', 'customs', 'excise',
        'withholding tax', 'tds', 'tcs', 'capital gains', 'tax rate', 'tax slab',
        'tax exemption', 'tax deduction', 'tax credit', 'tax relief', 'tax rebate',
        'filing', 'return', 'assessment', 'levy', 'duty', 'cess', 'surcharge',

        # Labor
        'minimum wage', 'wage revision', 'wage rate', 'salary', 'remuneration',
        'overtime', 'working hours', 'leave', 'holiday', 'bonus', 'gratuity',
        'termination', 'retrenchment', 'layoff', 'severance', 'notice period',
        'employment', 'labor code', 'labour law', 'industrial relations',

        # Social Security
        'provident fund', 'pension', 'superannuation', 'retirement', 'epf', 'ppf',
        'social security', 'insurance', 'esi', 'esic', 'health insurance',
        'contribution', 'employer contribution', 'employee contribution',

        # Compliance
        'compliance', 'statutory', 'mandatory', 'deadline', 'due date',
        'penalty', 'interest', 'fine', 'prosecution', 'enforcement',
        'registration', 'license', 'permit', 'approval',
    ])

    @classmethod
    def is_navigation_junk(cls, text: str, url: str) -> bool:
        """Check if text/url is navigation junk"""
        normalized = ' '.join(text.lower().strip().split())
        url_lower = url.lower()

        # Too short = probably navigation
        if len(normalized) < 8:
            return True

        # Exact match to blocklist
        if normalized in cls.NAVIGATION_BLOCKLIST:
            return True

        # Check for blocklist words at start
        for blocked in cls.NAVIGATION_BLOCKLIST:
            if normalized.startswith(blocked + ' ') or normalized.endswith(' ' + blocked):
                if len(normalized) < 40:
                    return True

        # URL patterns that indicate navigation
        nav_url_patterns = [
            '/about', '/contact', '/login', '/register', '/search', '/help',
            '/faq', '/privacy', '/terms', '/sitemap', '/careers', '/jobs',
            '/services', '/products', '?lang=', '&lang=', '#',
            'javascript:', 'mailto:', 'tel:',
        ]
        for pattern in nav_url_patterns:
            if pattern in url_lower and '.pdf' not in url_lower:
                return True

        return False

    @classmethod
    def calculate_relevance_score(cls, text: str, url: str) -> float:
        """
        Calculate relevance score (0.0 to 1.0)
        Higher = more likely to be a relevant document
        """
        score = 0.0
        text_lower = text.lower()
        url_lower = url.lower()

        # Is it a file? (+0.3)
        if Patterns.FILE_EXT.search(url_lower):
            score += 0.3

        # Has document ID? (+0.25)
        for pattern in Patterns.DOC_IDS:
            if pattern.search(text):
                score += 0.25
                break

        # Has date? (+0.15)
        for pattern in Patterns.DATES:
            if pattern.search(text):
                score += 0.15
                break

        # Has recent year? (+0.1)
        if Patterns.YEAR.search(text):
            score += 0.1

        # Document type keywords (+0.1 each, max 0.2)
        doc_keyword_count = sum(1 for kw in cls.DOCUMENT_KEYWORDS if kw in text_lower)
        score += min(doc_keyword_count * 0.1, 0.2)

        # Regulatory keywords (+0.05 each, max 0.15)
        reg_keyword_count = sum(1 for kw in cls.REGULATORY_KEYWORDS if kw in text_lower)
        score += min(reg_keyword_count * 0.05, 0.15)

        # URL contains document indicators (+0.1)
        url_indicators = ['/circular', '/notification', '/order', '/gazette', '/notice', 'download', 'attachment']
        if any(ind in url_lower for ind in url_indicators):
            score += 0.1

        # Length bonus (longer = more likely real document title)
        if len(text) > 30:
            score += 0.05
        if len(text) > 60:
            score += 0.05

        return min(score, 1.0)

    @classmethod
    def passes_filter(cls, text: str, url: str, min_score: float = 0.25) -> Tuple[bool, float]:
        """
        Main filter function.
        Returns: (passes: bool, score: float)
        """
        # First check if it's junk
        if cls.is_navigation_junk(text, url):
            return False, 0.0

        # Calculate relevance score
        score = cls.calculate_relevance_score(text, url)

        return score >= min_score, score

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# REPOSITORIES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

REPOSITORIES: List[Repository] = [
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # INDIA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://incometaxindia.gov.in/pages/communications/circulars.aspx",
        country="India",
        agency="Income Tax Department",
        doc_type="circular",
    ),
    Repository(
        url="https://incometaxindia.gov.in/pages/communications/notifications.aspx",
        country="India",
        agency="Income Tax Department",
        doc_type="notification",
    ),
    Repository(
        url="https://www.epfindia.gov.in/site_en/Circulars.php",
        country="India",
        agency="EPFO",
        doc_type="circular",
    ),
    Repository(
        url="https://www.esic.gov.in/circulars",
        country="India",
        agency="ESIC",
        doc_type="circular",
    ),
    Repository(
        url="https://labour.gov.in/whatsnew",
        country="India",
        agency="Ministry of Labour",
        doc_type="notification",
    ),
    Repository(
        url="https://www.cbic.gov.in/htdocs-cbec/customs/cs-circulars/cs-circulars-2024",
        country="India",
        agency="CBIC",
        doc_type="circular",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # UAE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.mohre.gov.ae/en/laws-and-regulations/resolutions-and-circulars.aspx",
        country="UAE",
        agency="MOHRE",
        doc_type="resolution",
    ),
    Repository(
        url="https://tax.gov.ae/en/content/guides.references.aspx",
        country="UAE",
        agency="Federal Tax Authority",
        doc_type="guide",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PHILIPPINES
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.bir.gov.ph/index.php/revenue-issuances/revenue-memorandum-circulars.html",
        country="Philippines",
        agency="BIR",
        doc_type="circular",
    ),
    Repository(
        url="https://www.bir.gov.ph/index.php/revenue-issuances/revenue-memorandum-orders.html",
        country="Philippines",
        agency="BIR",
        doc_type="order",
    ),
    Repository(
        url="https://www.bir.gov.ph/index.php/revenue-issuances/revenue-regulations.html",
        country="Philippines",
        agency="BIR",
        doc_type="regulation",
    ),
    Repository(
        url="https://www.dole.gov.ph/issuances/labor-advisories/",
        country="Philippines",
        agency="DOLE",
        doc_type="advisory",
    ),
    Repository(
        url="https://www.philhealth.gov.ph/circulars/",
        country="Philippines",
        agency="PhilHealth",
        doc_type="circular",
    ),
    Repository(
        url="https://www.sss.gov.ph/sss/appmanager/viewArticle.jsp?page=circulars",
        country="Philippines",
        agency="SSS",
        doc_type="circular",
    ),
    Repository(
        url="https://www.pagibigfund.gov.ph/circulars.html",
        country="Philippines",
        agency="Pag-IBIG",
        doc_type="circular",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # KENYA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.kra.go.ke/news-center/public-notices",
        country="Kenya",
        agency="KRA",
        doc_type="notice",
    ),
    Repository(
        url="https://www.nssf.or.ke/tenders-and-notices",
        country="Kenya",
        agency="NSSF",
        doc_type="notice",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # NIGERIA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.firs.gov.ng/press-release/",
        country="Nigeria",
        agency="FIRS",
        doc_type="press_release",
    ),
    Repository(
        url="https://pencom.gov.ng/category/circulars/",
        country="Nigeria",
        agency="PenCom",
        doc_type="circular",
    ),
    Repository(
        url="https://nsitf.gov.ng/news/",
        country="Nigeria",
        agency="NSITF",
        doc_type="news",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # GHANA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://gra.gov.gh/practice-notes/",
        country="Ghana",
        agency="GRA",
        doc_type="practice_note",
    ),
    Repository(
        url="https://www.ssnit.org.gh/news-events/",
        country="Ghana",
        agency="SSNIT",
        doc_type="news",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # SOUTH AFRICA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.sars.gov.za/legal-counsel/secondary-legislation/public-notices/",
        country="South Africa",
        agency="SARS",
        doc_type="notice",
    ),
    Repository(
        url="https://www.sars.gov.za/legal-counsel/secondary-legislation/binding-general-rulings/",
        country="South Africa",
        agency="SARS",
        doc_type="ruling",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # UGANDA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.ura.go.ug/publicNotices.do",
        country="Uganda",
        agency="URA",
        doc_type="notice",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ZAMBIA
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.zra.org.zm/tax-information/",
        country="Zambia",
        agency="ZRA",
        doc_type="information",
    ),

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ZIMBABWE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Repository(
        url="https://www.zimra.co.zw/public-notices",
        country="Zimbabwe",
        agency="ZIMRA",
        doc_type="notice",
    ),
]

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATABASE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Database:
    """SQLite database handler"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self._init_db()

    def _init_db(self):
        """Initialize database schema"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.row_factory = sqlite3.Row

        self.conn.executescript('''
            CREATE TABLE IF NOT EXISTS documents (
                url TEXT PRIMARY KEY,
                url_hash TEXT UNIQUE,
                country TEXT NOT NULL,
                agency TEXT,
                title TEXT,
                doc_id TEXT,
                date_found TEXT,
                date_published TEXT,
                relevance_score REAL,
                category TEXT,
                ai_summary TEXT,
                is_relevant INTEGER DEFAULT 0,
                content_hash TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE INDEX IF NOT EXISTS idx_country ON documents(country);
            CREATE INDEX IF NOT EXISTS idx_relevant ON documents(is_relevant);
            CREATE INDEX IF NOT EXISTS idx_created ON documents(created_at);
            CREATE INDEX IF NOT EXISTS idx_url_hash ON documents(url_hash);

            CREATE TABLE IF NOT EXISTS scan_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scan_date TEXT,
                countries_scanned INTEGER,
                repos_scanned INTEGER,
                docs_found INTEGER,
                docs_relevant INTEGER,
                duration_seconds REAL,
                errors TEXT
            );
        ''')
        self.conn.commit()

    def is_new_url(self, url: str) -> bool:
        """Check if URL has been seen"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cursor = self.conn.execute(
            'SELECT 1 FROM documents WHERE url_hash = ?', (url_hash,)
        )
        return cursor.fetchone() is None

    def bulk_check_urls(self, urls: List[str]) -> Set[str]:
        """Check multiple URLs at once, return set of new URLs"""
        if not urls:
            return set()

        url_hashes = {hashlib.md5(url.encode()).hexdigest(): url for url in urls}
        placeholders = ','.join('?' * len(url_hashes))

        cursor = self.conn.execute(
            f'SELECT url_hash FROM documents WHERE url_hash IN ({placeholders})',
            list(url_hashes.keys())
        )

        existing = {row[0] for row in cursor.fetchall()}
        new_urls = {url for hash_, url in url_hashes.items() if hash_ not in existing}

        return new_urls

    def save_document(self, doc: Document):
        """Save document to database"""
        url_hash = hashlib.md5(doc.url.encode()).hexdigest()
        content_hash = hashlib.md5((doc.content_snippet or '').encode()).hexdigest() if doc.content_snippet else None

        self.conn.execute('''
            INSERT OR REPLACE INTO documents
            (url, url_hash, country, agency, title, doc_id, date_found, date_published,
             relevance_score, category, ai_summary, is_relevant, content_hash, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        ''', (
            doc.url, url_hash, doc.country, doc.agency, doc.title, doc.doc_id,
            doc.date_found, doc.date_published, doc.relevance_score,
            doc.category.value if doc.category else None, doc.ai_summary,
            1 if doc.is_relevant else 0, content_hash
        ))
        self.conn.commit()

    def save_scan_log(self, countries: int, repos: int, found: int, relevant: int, duration: float, errors: str = None):
        """Log scan results"""
        self.conn.execute('''
            INSERT INTO scan_log (scan_date, countries_scanned, repos_scanned, docs_found, docs_relevant, duration_seconds, errors)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (datetime.now().isoformat(), countries, repos, found, relevant, duration, errors))
        self.conn.commit()

    def get_recent_stats(self, days: int = 7) -> Dict:
        """Get statistics for recent scans"""
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()

        cursor = self.conn.execute('''
            SELECT
                COUNT(*) as scan_count,
                SUM(docs_found) as total_found,
                SUM(docs_relevant) as total_relevant,
                AVG(duration_seconds) as avg_duration
            FROM scan_log
            WHERE scan_date > ?
        ''', (cutoff,))

        row = cursor.fetchone()
        return dict(row) if row else {}

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONTENT EXTRACTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ContentExtractor:
    """Extract content from PDFs and web pages"""

    @staticmethod
    async def extract_pdf_async(session: aiohttp.ClientSession, url: str) -> Tuple[str, bool]:
        """
        Extract text from PDF.
        Returns: (content, success)
        """
        if not PDF_SUPPORT:
            return "[PDF extraction not available]", False

        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=Config.REQUEST_TIMEOUT)) as response:
                if response.status != 200:
                    return f"[HTTP {response.status}]", False

                # Check size
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > Config.MAX_PDF_SIZE_MB * 1024 * 1024:
                    return "[PDF too large]", False

                content = await response.read()

                # Parse PDF
                pdf_file = io.BytesIO(content)
                reader = PdfReader(pdf_file)

                text = ""
                for i, page in enumerate(reader.pages[:Config.MAX_PDF_PAGES]):
                    extracted = page.extract_text()
                    if extracted:
                        text += extracted + "\n"

                if len(text.strip()) < 50:
                    return "[Scanned PDF - no extractable text]", False

                return text[:Config.MAX_CONTENT_LENGTH], True

        except Exception as e:
            return f"[PDF Error: {str(e)[:50]}]", False

    @staticmethod
    async def extract_webpage_async(session: aiohttp.ClientSession, url: str) -> Tuple[str, bool]:
        """
        Extract text from webpage.
        Returns: (content, success)
        """
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=Config.REQUEST_TIMEOUT)) as response:
                if response.status != 200:
                    return f"[HTTP {response.status}]", False

                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                # Remove junk elements
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'menu', 'noscript']):
                    tag.decompose()

                # Get text
                text = soup.get_text(separator=' ', strip=True)
                clean_text = ' '.join(text.split())

                return clean_text[:Config.MAX_CONTENT_LENGTH], True

        except Exception as e:
            return f"[Page Error: {str(e)[:50]}]", False

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# AI ANALYSIS (Gemini)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class GeminiAnalyzer:
    """Analyze documents using Gemini AI"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{Config.GEMINI_MODEL}:generateContent"

    async def analyze_document(self, doc: Document, session: aiohttp.ClientSession) -> Tuple[bool, str, DocumentCategory]:
        """
        Analyze a single document.
        Returns: (is_relevant, summary, category)
        """
        # Build prompt based on available content
        if doc.content_snippet and not doc.content_snippet.startswith('['):
            prompt = self._build_content_prompt(doc)
        else:
            prompt = self._build_title_prompt(doc)

        # Call API with retry
        for attempt in range(Config.GEMINI_RETRY_ATTEMPTS):
            try:
                url = f"{self.base_url}?key={self.api_key}"
                payload = {
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {
                        "temperature": 0.1,
                        "maxOutputTokens": 200,
                    }
                }

                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        result = await response.json()
                        answer = result['candidates'][0]['content']['parts'][0]['text'].strip()
                        return self._parse_response(answer)
                    elif response.status == 429:
                        # Rate limited, wait and retry
                        await asyncio.sleep(Config.GEMINI_RETRY_DELAY * (attempt + 1))
                    else:
                        return False, f"API Error: {response.status}", DocumentCategory.OTHER

            except Exception as e:
                if attempt < Config.GEMINI_RETRY_ATTEMPTS - 1:
                    await asyncio.sleep(Config.GEMINI_RETRY_DELAY)
                else:
                    return False, f"Error: {str(e)[:50]}", DocumentCategory.OTHER

        return False, "Max retries exceeded", DocumentCategory.OTHER

    def _build_content_prompt(self, doc: Document) -> str:
        return f"""You are a Payroll Compliance Auditor analyzing regulatory documents for {doc.country}.

DOCUMENT:
Title: "{doc.title}"
Agency: {doc.agency}
Content excerpt:
---
{doc.content_snippet[:2000]}
---

TASK: Determine if this is a relevant regulatory update for Payroll, Tax, or Labor Law.

RELEVANT topics include:
- Tax rates, slabs, exemptions, deductions, filing deadlines
- Social security contributions, pension rules, provident fund
- Minimum wage, bonus, gratuity, leave, working hours
- Compliance requirements, statutory deadlines, penalties

NOT RELEVANT: General news, tenders, job postings, organizational updates, event announcements

RESPOND IN THIS EXACT FORMAT:
RELEVANT: [YES/NO]
CATEGORY: [TAX/LABOR/PENSION/SOCIAL_SECURITY/COMPLIANCE/OTHER]
SUMMARY: [One sentence describing the key regulatory change/update]"""

    def _build_title_prompt(self, doc: Document) -> str:
        return f"""You are a Payroll Compliance Auditor analyzing regulatory documents for {doc.country}.

DOCUMENT TITLE: "{doc.title}"
Agency: {doc.agency}
(Note: Full content not available)

TASK: Based on the title, determine if this is likely a relevant regulatory update for Payroll, Tax, or Labor Law.

RELEVANT indicators in title:
- Circular/Notification/Order numbers
- Tax, duty, levy, rate references
- Social security, pension, provident fund
- Wage, salary, bonus, gratuity
- Compliance, deadline, filing references

NOT RELEVANT: Navigation links, general pages, tenders, jobs, events

RESPOND IN THIS EXACT FORMAT:
RELEVANT: [YES/NO]
CATEGORY: [TAX/LABOR/PENSION/SOCIAL_SECURITY/COMPLIANCE/OTHER]
SUMMARY: [One sentence describing what this document likely covers]"""

    def _parse_response(self, answer: str) -> Tuple[bool, str, DocumentCategory]:
        """Parse Gemini response"""
        lines = answer.strip().split('\n')

        is_relevant = False
        category = DocumentCategory.OTHER
        summary = "Analysis unavailable"

        for line in lines:
            line = line.strip()
            if line.upper().startswith('RELEVANT:'):
                is_relevant = 'YES' in line.upper()
            elif line.upper().startswith('CATEGORY:'):
                cat_str = line.split(':', 1)[1].strip().upper()
                category_map = {
                    'TAX': DocumentCategory.TAX,
                    'LABOR': DocumentCategory.LABOR,
                    'LABOUR': DocumentCategory.LABOR,
                    'PENSION': DocumentCategory.PENSION,
                    'SOCIAL_SECURITY': DocumentCategory.SOCIAL_SECURITY,
                    'COMPLIANCE': DocumentCategory.COMPLIANCE,
                }
                category = category_map.get(cat_str, DocumentCategory.OTHER)
            elif line.upper().startswith('SUMMARY:'):
                summary = line.split(':', 1)[1].strip()

        return is_relevant, summary, category

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TELEGRAM REPORTER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TelegramReporter:
    """Send reports to Telegram"""

    def __init__(self, token: str, chat_id: str):
        self.token = token
        self.chat_id = chat_id
        self.base_url = f"https://api.telegram.org/bot{token}"

    async def send_message(self, message: str) -> bool:
        """Send a message to Telegram"""
        if len(message) > 4000:
            message = message[:3900] + "\n\n... _(truncated)_"

        payload = {
            "chat_id": self.chat_id,
            "text": message,
            "parse_mode": "Markdown",
            "disable_web_page_preview": True,
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(f"{self.base_url}/sendMessage", json=payload, timeout=aiohttp.ClientTimeout(total=15)) as response:
                    return response.status == 200
        except:
            return False

    async def send_country_report(self, country: str, documents: List[Document]) -> bool:
        """Send report for a country"""
        if not documents:
            return True

        # Group by category
        by_category = defaultdict(list)
        for doc in documents:
            by_category[doc.category].append(doc)

        # Build message
        category_emojis = {
            DocumentCategory.TAX: "üí∞",
            DocumentCategory.LABOR: "üë∑",
            DocumentCategory.PENSION: "üè¶",
            DocumentCategory.SOCIAL_SECURITY: "üõ°Ô∏è",
            DocumentCategory.COMPLIANCE: "üìã",
            DocumentCategory.OTHER: "üìÑ",
        }

        msg = f"üö® *{country.upper()} - REGULATORY UPDATES*\n"
        msg += f"_{len(documents)} new document(s) found_\n\n"

        for category, docs in by_category.items():
            emoji = category_emojis.get(category, "üìÑ")
            msg += f"{emoji} *{category.value.replace('_', ' ').title()}*\n"

            for doc in docs[:5]:  # Max 5 per category
                safe_title = self._escape_markdown(doc.title[:80])
                safe_summary = self._escape_markdown(doc.ai_summary[:120]) if doc.ai_summary else "Analysis pending"

                msg += f"\n‚Ä¢ *{safe_title}*\n"
                if doc.doc_id:
                    msg += f"  üìù {doc.doc_id}\n"
                msg += f"  üí° {safe_summary}\n"
                if doc.date_published and doc.date_published != 'UNKNOWN':
                    msg += f"  üìÖ {doc.date_published}\n"
                msg += f"  [üîó Open]({doc.url})\n"

            msg += "\n"

        return await self.send_message(msg)

    async def send_summary(self, total_docs: int, total_relevant: int, countries: int, duration: float):
        """Send final summary"""
        msg = f"""‚úÖ *AUDIT COMPLETE*

üìä *Results:*
‚Ä¢ Documents analyzed: *{total_docs}*
‚Ä¢ Relevant updates found: *{total_relevant}*
‚Ä¢ Countries scanned: *{countries}*
‚Ä¢ Duration: *{duration:.1f}* seconds

üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}"""

        await self.send_message(msg)

    def _escape_markdown(self, text: str) -> str:
        """Escape markdown special characters"""
        for char in ['*', '_', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!']:
            text = text.replace(char, f'\\{char}')
        return text

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN SCANNER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PayrollAuditScanner:
    """Main scanner orchestrator"""

    def __init__(self):
        self.db = Database(Config.DB_PATH)
        self.analyzer = GeminiAnalyzer(Config.GEMINI_API_KEY)
        self.reporter = TelegramReporter(Config.TELEGRAM_TOKEN, Config.TELEGRAM_CHAT_ID)
        self.rate_limiter = defaultdict(float)  # domain -> last_request_time

    async def run(self):
        """Run the full audit"""
        start_time = time.time()

        print("=" * 70)
        print("üöÄ PAYROLL REGULATORY AUDIT AGENT v7.0")
        print("=" * 70)
        print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üîç Scanning {len(REPOSITORIES)} repositories across {len(set(r.country for r in REPOSITORIES))} countries")
        print()

        # SSL context for aiohttp
        ssl_context = False  # Disable SSL verification for government sites

        connector = aiohttp.TCPConnector(ssl=ssl_context, limit=Config.MAX_CONCURRENT_REQUESTS)
        timeout = aiohttp.ClientTimeout(total=Config.REQUEST_TIMEOUT)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # Scan all repositories
            all_documents = await self._scan_all_repositories(session)

            print(f"\nüìä Found {len(all_documents)} potential documents")

            # Filter to new documents only
            all_urls = [doc.url for doc in all_documents]
            new_urls = self.db.bulk_check_urls(all_urls)
            new_documents = [doc for doc in all_documents if doc.url in new_urls]

            print(f"üÜï {len(new_documents)} are new (not seen before)")

            if not new_documents:
                print("\n‚úÖ No new documents to analyze")
                await self.reporter.send_message("‚úÖ *AUDIT COMPLETE*\n\nNo new regulatory updates found.")
                return

            # Analyze with Gemini
            print(f"\nü§ñ Analyzing with Gemini AI...")
            relevant_documents = await self._analyze_documents(session, new_documents)

            print(f"‚úÖ {len(relevant_documents)} relevant documents identified")

            # Save all documents to database
            for doc in new_documents:
                self.db.save_document(doc)

            # Send reports
            if relevant_documents:
                print("\nüì§ Sending reports...")
                await self._send_reports(relevant_documents)

            # Final summary
            duration = time.time() - start_time
            countries_with_updates = len(set(doc.country for doc in relevant_documents))

            await self.reporter.send_summary(
                total_docs=len(new_documents),
                total_relevant=len(relevant_documents),
                countries=countries_with_updates,
                duration=duration
            )

            # Log scan
            self.db.save_scan_log(
                countries=len(set(r.country for r in REPOSITORIES)),
                repos=len(REPOSITORIES),
                found=len(new_documents),
                relevant=len(relevant_documents),
                duration=duration
            )

            print(f"\n‚úÖ AUDIT COMPLETE in {duration:.1f} seconds")

    async def _scan_all_repositories(self, session: aiohttp.ClientSession) -> List[Document]:
        """Scan all repositories concurrently"""
        tasks = []
        for repo in REPOSITORIES:
            task = self._scan_repository(session, repo)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_documents = []
        for result in results:
            if isinstance(result, list):
                all_documents.extend(result)
            elif isinstance(result, Exception):
                print(f"   ‚ö†Ô∏è Error: {str(result)[:50]}")

        return all_documents

    async def _scan_repository(self, session: aiohttp.ClientSession, repo: Repository) -> List[Document]:
        """Scan a single repository"""
        print(f"   üìç {repo.country} - {repo.agency}...")

        # Rate limiting per domain
        domain = urlparse(repo.url).netloc
        await self._rate_limit(domain)

        try:
            async with session.get(repo.url) as response:
                if response.status != 200:
                    print(f"      ‚ö†Ô∏è HTTP {response.status}")
                    return []

                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                documents = []

                for link in soup.find_all('a', href=True):
                    href = link.get('href', '').strip()
                    text = link.get_text(separator=' ', strip=True)

                    if not href or not text or len(text) < 5:
                        continue

                    # Build full URL
                    full_url = urljoin(repo.url, href)

                    # Skip external domains (except for PDFs)
                    link_domain = urlparse(full_url).netloc
                    if link_domain != domain and not full_url.lower().endswith('.pdf'):
                        continue

                    # Skip anchors and javascript
                    if href.startswith('#') or href.startswith('javascript:'):
                        continue

                    # Apply filters
                    passes, score = Filters.passes_filter(text, full_url)

                    if passes:
                        # Extract document ID
                        doc_id = None
                        for pattern in Patterns.DOC_IDS:
                            match = pattern.search(text)
                            if match:
                                doc_id = match.group(1) if match.groups() else match.group(0)
                                break

                        # Extract date
                        date_published = None
                        for pattern in Patterns.DATES:
                            match = pattern.search(text)
                            if match:
                                date_published = match.group(0)
                                break

                        doc = Document(
                            url=full_url,
                            title=text[:200],
                            country=repo.country,
                            agency=repo.agency,
                            date_found=datetime.now().isoformat(),
                            date_published=date_published,
                            doc_id=doc_id,
                            is_pdf=full_url.lower().endswith('.pdf'),
                            relevance_score=score,
                        )
                        documents.append(doc)

                print(f"      ‚úì Found {len(documents)} candidates")
                return documents[:Config.MAX_DOCUMENTS_PER_REPO]

        except Exception as e:
            print(f"      ‚ö†Ô∏è Error: {str(e)[:50]}")
            return []

    async def _analyze_documents(self, session: aiohttp.ClientSession, documents: List[Document]) -> List[Document]:
        """Analyze documents with Gemini"""
        relevant = []

        # Process in batches
        for i in range(0, len(documents), Config.GEMINI_BATCH_SIZE):
            batch = documents[i:i + Config.GEMINI_BATCH_SIZE]

            # Extract content for batch
            content_tasks = []
            for doc in batch:
                if doc.is_pdf:
                    task = ContentExtractor.extract_pdf_async(session, doc.url)
                else:
                    task = ContentExtractor.extract_webpage_async(session, doc.url)
                content_tasks.append(task)

            contents = await asyncio.gather(*content_tasks, return_exceptions=True)

            # Update documents with content
            for doc, content_result in zip(batch, contents):
                if isinstance(content_result, tuple):
                    doc.content_snippet, _ = content_result
                else:
                    doc.content_snippet = "[Error extracting content]"

            # Analyze with Gemini
            analysis_tasks = []
            for doc in batch:
                task = self.analyzer.analyze_document(doc, session)
                analysis_tasks.append(task)

            analyses = await asyncio.gather(*analysis_tasks, return_exceptions=True)

            # Update documents with analysis
            for doc, analysis_result in zip(batch, analyses):
                if isinstance(analysis_result, tuple):
                    is_rel, summary, category = analysis_result
                    doc.is_relevant = is_rel
                    doc.ai_summary = summary
                    doc.category = category

                    if is_rel:
                        relevant.append(doc)
                        print(f"      ‚úÖ {doc.title[:50]}...")
                    else:
                        print(f"      ‚ùå {doc.title[:50]}...")

            # Small delay between batches
            await asyncio.sleep(0.5)

        return relevant

    async def _send_reports(self, documents: List[Document]):
        """Send reports grouped by country"""
        # Group by country
        by_country = defaultdict(list)
        for doc in documents:
            by_country[doc.country].append(doc)

        # Send report for each country
        for country, docs in by_country.items():
            success = await self.reporter.send_country_report(country, docs)
            if success:
                print(f"      ‚úÖ Sent {len(docs)} updates for {country}")
            else:
                print(f"      ‚ö†Ô∏è Failed to send report for {country}")
            await asyncio.sleep(1)  # Rate limit Telegram

    async def _rate_limit(self, domain: str):
        """Rate limit requests per domain"""
        last_request = self.rate_limiter[domain]
        elapsed = time.time() - last_request

        if elapsed < Config.RATE_LIMIT_PER_DOMAIN:
            await asyncio.sleep(Config.RATE_LIMIT_PER_DOMAIN - elapsed)

        self.rate_limiter[domain] = time.time()

    def close(self):
        """Cleanup"""
        self.db.close()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENTRY POINT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

async def main():
    """Main entry point"""
    if not Config.validate():
        return

    scanner = PayrollAuditScanner()
    try:
        await scanner.run()
    finally:
        scanner.close()

if __name__ == "__main__":
    asyncio.run(main())
